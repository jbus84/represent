#!/usr/bin/env python
"""
Complete Workflow Demo: Three Core Modules

This demo showcases the complete represent workflow using all three core modules:
1. Global Threshold Calculator - Calculate classification thresholds
2. Dataset Builder - Create comprehensive symbol datasets
3. Market Depth Processor - Visualize market depth features

The demo creates a unified HTML report showing the complete process.
"""

import sys
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import polars as pl

# Add represent package to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from represent import (
    DatasetBuildConfig,
    build_datasets_from_dbn_files,
    calculate_global_thresholds,
    process_market_data,
)
from represent.configs import (
    DatasetBuilderConfig,
    GlobalThresholdConfig,
    MarketDepthProcessorConfig,
    create_compatible_configs,
)


class WorkflowDemo:
    """Complete workflow demonstration with unified reporting."""

    def __init__(self):
        self.output_dir = Path("examples/complete_workflow_output")
        self.output_dir.mkdir(exist_ok=True)
        self.report_sections = []

    def log_section(self, title, content):
        """Add a section to the report."""
        print(f"\n{title}")
        print("=" * len(title))
        print(content)

        self.report_sections.append({
            'title': title,
            'content': content,
            'timestamp': datetime.now().strftime("%H:%M:%S")
        })

    def step1_global_thresholds(self):
        """Step 1: Demonstrate Global Threshold Calculator with focused configuration."""
        self.log_section(
            "üéØ STEP 1: Global Threshold Calculation",
            "Demonstrating the new focused configuration architecture for threshold calculation..."
        )

        # NEW APPROACH: Use focused GlobalThresholdConfig
        threshold_config = GlobalThresholdConfig(
            currency="AUDUSD",
            nbins=13,
            lookback_rows=1000,
            lookforward_input=1000,
            lookforward_offset=100,
            max_samples_per_file=10000,
            sample_fraction=0.5,
            jump_size=100
        )

        # Show configuration details
        config_info = f"""
üÜï NEW FOCUSED CONFIGURATION APPROACH:

üìä GlobalThresholdConfig - Only threshold-specific parameters:
   ‚Ä¢ Currency: {threshold_config.currency}
   ‚Ä¢ Number of bins: {threshold_config.nbins}
   ‚Ä¢ Lookback rows: {threshold_config.lookback_rows}
   ‚Ä¢ Lookforward input: {threshold_config.lookforward_input}
   ‚Ä¢ Lookforward offset: {threshold_config.lookforward_offset}
   ‚Ä¢ Max samples per file: {threshold_config.max_samples_per_file}
   ‚Ä¢ Sample fraction: {threshold_config.sample_fraction}
   ‚Ä¢ Jump size: {threshold_config.jump_size}

‚úÖ Benefits of focused configuration:
   ‚Ä¢ Only parameters relevant to threshold calculation
   ‚Ä¢ Clear separation of concerns
   ‚Ä¢ Better type safety and validation
   ‚Ä¢ No confusion with unrelated parameters
"""

        self.log_section("üîß Configuration Architecture", config_info)

        # Look for DBN files in the data directory
        data_paths = [
            Path("data/databento/AUDUSD/"),
            Path("data/"),
            Path("examples/data/"),
        ]

        dbn_files = []
        for data_path in data_paths:
            if data_path.exists():
                dbn_files = list(data_path.glob("*.dbn*"))
                if dbn_files:
                    break

        if not dbn_files:
            # Create mock threshold data for demo purposes
            self.log_section(
                "‚ÑπÔ∏è  Mock Threshold Generation",
                "No DBN files found, generating mock thresholds for demonstration..."
            )

            # Create mock thresholds object
            from represent.global_threshold_calculator import GlobalThresholds

            # Generate realistic price movement distribution
            np.random.seed(42)
            mock_movements = np.random.normal(0, 0.001, 50000)  # 0.1% std movement

            # Calculate quantile boundaries using the focused config
            quantiles = np.linspace(0, 1, threshold_config.nbins + 1)
            boundaries = np.quantile(mock_movements, quantiles)

            thresholds = GlobalThresholds(
                quantile_boundaries=boundaries,
                nbins=threshold_config.nbins,
                sample_size=50000,
                files_analyzed=1,
                price_movement_stats={
                    'mean': float(mock_movements.mean()),
                    'std': float(mock_movements.std()),
                    'min': float(mock_movements.min()),
                    'max': float(mock_movements.max())
                }
            )

            results = {
                'thresholds': thresholds,
                'sample_files': ['mock_data_generation'],
                'total_movements': 50000,
                'config': threshold_config
            }

        else:
            try:
                # Calculate real thresholds using the focused configuration
                thresholds = calculate_global_thresholds(
                    config=threshold_config,
                    data_directory=dbn_files[0].parent,
                    sample_fraction=threshold_config.sample_fraction,
                    verbose=True
                )

                results = {
                    'thresholds': thresholds,
                    'sample_files': [f.name for f in dbn_files[:3]],  # Show first 3
                    'total_movements': thresholds.sample_size,
                    'config': threshold_config
                }

            except Exception as e:
                self.log_section("‚ùå Threshold Calculation Error", str(e))
                return None

        # Log threshold results
        threshold_info = f"""
‚úÖ Global thresholds calculated successfully using focused GlobalThresholdConfig!

üìä Threshold Statistics:
   ‚Ä¢ Number of bins: {results['thresholds'].nbins}
   ‚Ä¢ Sample size: {results['total_movements']:,} price movements
   ‚Ä¢ Files analyzed: {results['thresholds'].files_analyzed}
   ‚Ä¢ Currency: {threshold_config.currency}
   ‚Ä¢ Lookback rows: {threshold_config.lookback_rows}
   ‚Ä¢ Lookforward input: {threshold_config.lookforward_input}
   ‚Ä¢ Sample fraction: {threshold_config.sample_fraction}

üìÅ Source files analyzed: {len(results['sample_files'])}
   {chr(10).join(f'   ‚Ä¢ {f}' for f in results['sample_files'][:5])}
   {'   ‚Ä¢ ... and more' if len(results['sample_files']) > 5 else ''}

üéØ Classification boundaries:
   {chr(10).join(f'   Bin {i:2d}: {boundary:+.6f}' for i, boundary in enumerate(results['thresholds'].quantile_boundaries[:5]))}
   {'   ... (showing first 5)' if len(results['thresholds'].quantile_boundaries) > 5 else ''}

üÜï Configuration approach: Using focused GlobalThresholdConfig with only relevant parameters
"""

        self.log_section("üìä Threshold Results", threshold_info)

        # Create threshold visualization
        self._visualize_thresholds(results['thresholds'])

        return results

    def _visualize_thresholds(self, thresholds):
        """Create threshold distribution visualization."""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # Plot 1: Threshold boundaries
        boundaries = np.array(thresholds.quantile_boundaries)
        bin_centers = (boundaries[:-1] + boundaries[1:]) / 2

        ax1.bar(range(len(bin_centers)), np.ones(len(bin_centers)),
                color='steelblue', alpha=0.7, edgecolor='darkblue')
        ax1.set_title('Classification Bins (Uniform Distribution)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Classification Bin', fontsize=12)
        ax1.set_ylabel('Expected Frequency', fontsize=12)
        ax1.set_xticks(range(0, len(bin_centers), 2))
        ax1.grid(True, alpha=0.3)

        # Plot 2: Boundary values
        ax2.plot(boundaries, 'o-', color='darkred', linewidth=2, markersize=6)
        ax2.set_title('Threshold Boundary Values', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Boundary Index', fontsize=12)
        ax2.set_ylabel('Price Movement Threshold', fontsize=12)
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)

        plt.tight_layout()

        # Save threshold plot
        threshold_path = self.output_dir / "step1_global_thresholds.png"
        fig.savefig(threshold_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)

        self.threshold_plot_path = threshold_path
        return threshold_path

    def step2_dataset_building(self, threshold_results):
        """Step 2: Demonstrate Dataset Builder with focused configuration."""
        if not threshold_results:
            self.log_section("‚ùå Step 2 Skipped", "Cannot proceed without thresholds from Step 1")
            return None

        self.log_section(
            "üèóÔ∏è STEP 2: Dataset Building",
            "Demonstrating the focused DatasetBuilderConfig for symbol dataset creation..."
        )

        # NEW APPROACH: Use focused DatasetBuilderConfig
        dataset_builder_config = DatasetBuilderConfig(
            currency="AUDUSD",
            lookback_rows=1000,
            lookforward_input=1000,
            lookforward_offset=100
        )

        # Show the configuration benefits
        builder_config_info = f"""
üÜï NEW FOCUSED DATASET BUILDER CONFIGURATION:

üèóÔ∏è DatasetBuilderConfig - Only dataset building parameters:
   ‚Ä¢ Currency: {dataset_builder_config.currency}
   ‚Ä¢ Lookback rows: {dataset_builder_config.lookback_rows}
   ‚Ä¢ Lookforward input: {dataset_builder_config.lookforward_input}
   ‚Ä¢ Lookforward offset: {dataset_builder_config.lookforward_offset}
   ‚Ä¢ Min required samples: {dataset_builder_config.lookback_rows + dataset_builder_config.lookforward_input + dataset_builder_config.lookforward_offset}

‚úÖ Benefits of focused DatasetBuilderConfig:
   ‚Ä¢ Only parameters needed for dataset building
   ‚Ä¢ Computed field for min_required_samples
   ‚Ä¢ Clear separation from feature processing parameters
   ‚Ä¢ Better validation and type safety

üîÑ DatasetBuildConfig for dataset management:
   ‚Ä¢ Handles classification method (thresholds vs uniform)
   ‚Ä¢ Manages intermediate file cleanup
   ‚Ä¢ Controls output formatting
"""

        self.log_section("üîß Dataset Builder Configuration", builder_config_info)

        # Create dataset build configuration using calculated thresholds
        dataset_config = DatasetBuildConfig(
            currency="AUDUSD",
            global_thresholds=threshold_results['thresholds'],
            force_uniform=True,
            keep_intermediate=False,
            nbins=threshold_results['thresholds'].nbins
        )

        # Look for DBN files
        data_paths = [
            Path("data/databento/AUDUSD/"),
            Path("data/"),
            Path("examples/data/"),
        ]

        dbn_files = []
        for data_path in data_paths:
            if data_path.exists():
                dbn_files = list(data_path.glob("*.dbn*"))[:3]  # Use first 3 files
                if dbn_files:
                    break

        if not dbn_files:
            # Create mock dataset for demonstration
            self.log_section(
                "‚ÑπÔ∏è  Mock Dataset Generation",
                "No DBN files found, generating mock symbol dataset for demonstration..."
            )

            # Generate realistic mock market data using focused config
            mock_dataset = self._generate_mock_dataset(dataset_builder_config, threshold_results['thresholds'])

            # Save mock dataset
            dataset_path = self.output_dir / "AUDUSD_M6AM4_dataset.parquet"
            mock_dataset.write_parquet(dataset_path)

            results = {
                'datasets_created': {'AUDUSD_M6AM4_dataset.parquet': dataset_path},
                'total_samples': len(mock_dataset),
                'symbols': ['M6AM4'],
                'source_files': ['mock_generation']
            }

        else:
            try:
                # Build real datasets using focused DatasetBuilderConfig
                build_results = build_datasets_from_dbn_files(
                    config=dataset_builder_config,
                    dbn_files=dbn_files,
                    output_dir=str(self.output_dir),
                    dataset_config=dataset_config,
                    verbose=True
                )

                # Convert results to our format
                results = {
                    'datasets_created': {},
                    'total_samples': build_results.get('phase_2_stats', {}).get('total_samples', 0),
                    'symbols': [],
                    'source_files': [f.name for f in dbn_files]
                }

                # Find created datasets
                for dataset_file in self.output_dir.glob("*_dataset.parquet"):
                    results['datasets_created'][dataset_file.name] = dataset_file
                    symbol = dataset_file.name.split('_')[1]  # Extract symbol from filename
                    results['symbols'].append(symbol)

            except Exception as e:
                self.log_section("‚ùå Dataset Building Error", str(e))
                return None

        # Log dataset results
        dataset_info = f"""
‚úÖ Symbol datasets created successfully!

üìä Dataset Statistics:
   ‚Ä¢ Datasets created: {len(results['datasets_created'])}
   ‚Ä¢ Total samples: {results['total_samples']:,}
   ‚Ä¢ Symbols processed: {len(results['symbols'])}
   ‚Ä¢ Classification: {dataset_config.nbins} bins
   ‚Ä¢ Uniform distribution: {dataset_config.force_uniform}

üìÅ Output datasets:
   {chr(10).join(f'   ‚Ä¢ {name} ({path.stat().st_size // 1024} KB)' for name, path in results['datasets_created'].items())}

üéØ Symbols available:
   {', '.join(results['symbols']) if results['symbols'] else 'M6AM4 (mock)'}

üìÅ Source files: {len(results['source_files'])}
   {chr(10).join(f'   ‚Ä¢ {f}' for f in results['source_files'])}
"""

        self.log_section("üèóÔ∏è Dataset Results", dataset_info)

        # Create dataset analysis visualization
        if results['datasets_created']:
            first_dataset = list(results['datasets_created'].values())[0]
            self._analyze_dataset(first_dataset)

        return results

    def _generate_mock_dataset(self, config, thresholds):
        """Generate mock dataset for demonstration."""
        np.random.seed(42)
        n_samples = 5000

        # Generate timestamps
        timestamps = range(n_samples)

        # Generate price data
        base_price = 0.6600
        prices = [base_price + np.random.normal(0, 0.0001, 20) for _ in range(n_samples)]

        # Create bid/ask price columns
        data = {'ts_event': timestamps}

        for i in range(10):
            bid_prices = [price_array[i] - 0.0001 for price_array in prices]
            ask_prices = [price_array[i] + 0.0001 for price_array in prices]

            data[f'bid_px_{i:02d}'] = bid_prices
            data[f'ask_px_{i:02d}'] = ask_prices
            data[f'bid_sz_{i:02d}'] = np.random.exponential(1000, n_samples).astype(int)
            data[f'ask_sz_{i:02d}'] = np.random.exponential(1000, n_samples).astype(int)

        # Add classification labels using the thresholds
        np.random.seed(42)
        classifications = np.random.randint(0, thresholds.nbins, n_samples)
        data['classification_label'] = classifications

        return pl.DataFrame(data)

    def _analyze_dataset(self, dataset_path):
        """Analyze and visualize dataset."""
        df = pl.read_parquet(dataset_path)

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # Plot 1: Sample count over time
        ax1.plot(range(len(df)), color='steelblue', linewidth=1)
        ax1.set_title('Dataset Size Over Time', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Sample Index', fontsize=12)
        ax1.set_ylabel('Cumulative Samples', fontsize=12)
        ax1.grid(True, alpha=0.3)

        # Plot 2: Classification distribution
        if 'classification_label' in df.columns:
            class_counts = df['classification_label'].value_counts().sort('classification_label')
            ax2.bar(class_counts['classification_label'], class_counts['count'],
                   color='darkgreen', alpha=0.7)
            ax2.set_title('Classification Distribution', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Classification Bin', fontsize=12)
            ax2.set_ylabel('Sample Count', fontsize=12)
            ax2.grid(True, alpha=0.3)

        # Plot 3: Price spread analysis
        if 'bid_px_00' in df.columns and 'ask_px_00' in df.columns:
            spreads = df['ask_px_00'] - df['bid_px_00']
            ax3.hist(spreads, bins=50, color='orange', alpha=0.7, edgecolor='darkorange')
            ax3.set_title('Bid-Ask Spread Distribution', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Spread (Price Units)', fontsize=12)
            ax3.set_ylabel('Frequency', fontsize=12)
            ax3.grid(True, alpha=0.3)

        # Plot 4: Data quality metrics
        quality_metrics = {
            'Total Samples': len(df),
            'Columns': len(df.columns),
            'Price Levels': len([col for col in df.columns if '_px_' in col]),
            'Size Columns': len([col for col in df.columns if '_sz_' in col])
        }

        ax4.bar(range(len(quality_metrics)), list(quality_metrics.values()),
               color=['blue', 'green', 'red', 'purple'], alpha=0.7)
        ax4.set_title('Dataset Quality Metrics', fontsize=14, fontweight='bold')
        ax4.set_xticks(range(len(quality_metrics)))
        ax4.set_xticklabels(list(quality_metrics.keys()), rotation=45)
        ax4.set_ylabel('Count', fontsize=12)
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        # Save dataset analysis plot
        analysis_path = self.output_dir / "step2_dataset_analysis.png"
        fig.savefig(analysis_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)

        self.dataset_plot_path = analysis_path
        return analysis_path

    def step3_market_depth_visualization(self, dataset_results):
        """Step 3: Demonstrate Market Depth Processor with focused configuration."""
        if not dataset_results or not dataset_results['datasets_created']:
            self.log_section("‚ùå Step 3 Skipped", "Cannot proceed without datasets from Step 2")
            return None

        self.log_section(
            "‚ö° STEP 3: Market Depth Processing & Visualization",
            "Demonstrating the focused MarketDepthProcessorConfig for tensor generation..."
        )

        # Get the first available dataset
        first_dataset_path = list(dataset_results['datasets_created'].values())[0]
        df = pl.read_parquet(first_dataset_path)

        # NEW APPROACH: Use focused MarketDepthProcessorConfig
        processor_config = MarketDepthProcessorConfig(
            features=['volume', 'variance', 'trade_counts'],
            samples=min(25000, len(df)),  # Use available samples or default
            ticks_per_bin=100,
            micro_pip_size=0.00001
        )

        # Show configuration details
        processor_config_info = f"""
üÜï NEW FOCUSED MARKET DEPTH PROCESSOR CONFIGURATION:

‚ö° MarketDepthProcessorConfig - Only feature processing parameters:
   ‚Ä¢ Features: {processor_config.features}
   ‚Ä¢ Samples: {processor_config.samples:,}
   ‚Ä¢ Ticks per bin: {processor_config.ticks_per_bin}
   ‚Ä¢ Micro pip size: {processor_config.micro_pip_size}
   ‚Ä¢ Computed time bins: {processor_config.samples // processor_config.ticks_per_bin}
   ‚Ä¢ Computed output shape: {processor_config.output_shape if hasattr(processor_config, 'output_shape') else 'Computed at runtime'}

‚úÖ Benefits of focused MarketDepthProcessorConfig:
   ‚Ä¢ Only parameters needed for tensor generation
   ‚Ä¢ Automatic time_bins calculation
   ‚Ä¢ Computed output_shape field
   ‚Ä¢ Clear separation from dataset building parameters
   ‚Ä¢ Feature-specific validation

üéØ Output tensor shape for {len(processor_config.features)} features:
   ‚Ä¢ {'2D tensor' if len(processor_config.features) == 1 else '3D tensor'}: {processor_config.output_shape if hasattr(processor_config, 'output_shape') else 'Shape computed at runtime'}
"""

        self.log_section("üîß Market Depth Processor Configuration", processor_config_info)

        # Take sufficient samples for processing
        sample_size = min(5000, len(df))
        sample_data = df.head(sample_size)

        self.log_section(
            "üîÑ Feature Processing",
            f"Processing {sample_size} samples with features: {processor_config.features}"
        )

        try:
            # Generate multi-feature tensor using focused configuration
            multi_feature_tensor = process_market_data(
                sample_data,
                config=processor_config,
                features=processor_config.features
            )

            if multi_feature_tensor is not None and multi_feature_tensor.size > 0:
                feature_info = f"""
‚úÖ Market depth processing successful!

üìä Tensor Information:
   ‚Ä¢ Shape: {multi_feature_tensor.shape}
   ‚Ä¢ Data type: {multi_feature_tensor.dtype}
   ‚Ä¢ Memory usage: {multi_feature_tensor.nbytes / 1024:.1f} KB
   ‚Ä¢ Value range: [{multi_feature_tensor.min():.6f}, {multi_feature_tensor.max():.6f}]

üéØ Feature Analysis:
   ‚Ä¢ Non-zero elements: {np.count_nonzero(multi_feature_tensor):,}/{multi_feature_tensor.size:,} ({100*np.count_nonzero(multi_feature_tensor)/multi_feature_tensor.size:.1f}%)
   ‚Ä¢ Negative values: {np.sum(multi_feature_tensor < 0):,}
   ‚Ä¢ Positive values: {np.sum(multi_feature_tensor > 0):,}
   ‚Ä¢ Zero values: {np.sum(multi_feature_tensor == 0):,}
"""

                self.log_section("‚ö° Processing Results", feature_info)

                # Create comprehensive visualizations
                self._create_market_depth_visualizations(multi_feature_tensor, processor_config.features)

                results = {
                    'tensor_shape': multi_feature_tensor.shape,
                    'features': processor_config.features,
                    'samples_processed': sample_size,
                    'memory_usage_kb': multi_feature_tensor.nbytes / 1024,
                    'config': processor_config,
                    'success': True
                }

            else:
                self.log_section("‚ùå Processing Failed", "Unable to generate tensor from market data")
                return None

        except Exception as e:
            self.log_section("‚ùå Market Depth Processing Error", str(e))
            return None

        return results

    def _create_market_depth_visualizations(self, tensor, features):
        """Create comprehensive market depth visualizations."""
        # Individual feature plots
        feature_plots = []

        for i, feature in enumerate(features):
            if len(tensor.shape) == 3:
                feature_array = tensor[i]
            elif len(tensor.shape) == 2 and len(features) == 1:
                feature_array = tensor
            else:
                continue

            # Create individual feature visualization
            fig, ax = plt.subplots(figsize=(14, 8))
            im = ax.imshow(feature_array, cmap='RdBu', aspect='auto', interpolation='bilinear')
            ax.set_title(f"Market Depth Feature: {feature.upper()}", fontsize=18, fontweight='bold')
            ax.set_xlabel(f"Time Bins ({feature_array.shape[1]} total)", fontsize=14)
            ax.set_ylabel("Price Levels (402 total: 200 Ask + 2 Mid + 200 Bid)", fontsize=14)

            # Add grid and styling
            ax.set_xticks(range(0, feature_array.shape[1], max(1, feature_array.shape[1]//10)))
            ax.set_yticks(range(0, 402, 50))
            ax.grid(True, alpha=0.3, linestyle='--')

            # Enhanced colorbar
            cbar = fig.colorbar(im, shrink=0.8)
            cbar.set_label(f"Normalized {feature.title()} Value", fontsize=12, fontweight='bold')

            plt.tight_layout()

            # Save individual plot
            plot_path = self.output_dir / f"step3_{feature}_feature.png"
            fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close(fig)
            feature_plots.append(plot_path)

        # Create RGB combination if we have 3 features
        if len(features) >= 3 and len(tensor.shape) == 3:
            self._create_rgb_combination(tensor, features)

        # Create summary visualization
        self._create_feature_summary(tensor, features)

        self.feature_plots = feature_plots

    def _create_rgb_combination(self, tensor, features):
        """Create RGB combination visualization."""
        def normalize_for_rgb(arr):
            arr_min, arr_max = arr.min(), arr.max()
            if arr_max > arr_min:
                return (arr - arr_min) / (arr_max - arr_min)
            else:
                return np.zeros_like(arr)

        # Get first three features for RGB
        red_norm = normalize_for_rgb(tensor[0])    # volume
        green_norm = normalize_for_rgb(tensor[1])  # variance
        blue_norm = normalize_for_rgb(tensor[2])   # trade_counts

        # Create RGB array
        rgb_array = np.stack([red_norm, green_norm, blue_norm], axis=-1)

        # Create RGB visualization
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

        # RGB combination
        ax1.imshow(rgb_array, aspect='auto', interpolation='bilinear')
        ax1.set_title(f"RGB Feature Combination\n(Red={features[0]}, Green={features[1]}, Blue={features[2]})",
                     fontsize=16, fontweight='bold')
        ax1.set_xlabel(f"Time Bins ({rgb_array.shape[1]} total)", fontsize=14)
        ax1.set_ylabel("Price Levels (402 total)", fontsize=14)
        ax1.grid(True, alpha=0.3, linestyle='--')

        # Channel intensities over time
        channel_data = np.stack([
            red_norm.mean(axis=0),
            green_norm.mean(axis=0),
            blue_norm.mean(axis=0)
        ])

        im2 = ax2.imshow(channel_data, cmap='RdBu', aspect='auto')
        ax2.set_title("Average Channel Intensities", fontsize=16, fontweight='bold')
        ax2.set_xlabel("Time Bins", fontsize=14)
        ax2.set_ylabel("RGB Channels", fontsize=14)
        ax2.set_yticks([0, 1, 2])
        ax2.set_yticklabels([f'{features[i]} ({c})' for i, c in enumerate(['Red', 'Green', 'Blue'])])

        cbar = fig.colorbar(im2, ax=ax2, shrink=0.8)
        cbar.set_label("Normalized Intensity", fontsize=12)

        plt.tight_layout()

        # Save RGB combination
        rgb_path = self.output_dir / "step3_rgb_combination.png"
        fig.savefig(rgb_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)

        self.rgb_plot_path = rgb_path

    def _create_feature_summary(self, tensor, features):
        """Create feature summary visualization."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()

        # Plot 1: Feature statistics
        if len(tensor.shape) == 3:
            stats = []
            for i, feature in enumerate(features):
                feature_data = tensor[i]
                stats.append({
                    'Feature': feature,
                    'Mean': feature_data.mean(),
                    'Std': feature_data.std(),
                    'Min': feature_data.min(),
                    'Max': feature_data.max()
                })

            feature_names = [s['Feature'] for s in stats]
            means = [s['Mean'] for s in stats]
            stds = [s['Std'] for s in stats]

            x_pos = range(len(feature_names))
            axes[0].bar(x_pos, means, yerr=stds, capsize=5, color='steelblue', alpha=0.7)
            axes[0].set_title('Feature Statistics', fontsize=14, fontweight='bold')
            axes[0].set_xticks(x_pos)
            axes[0].set_xticklabels(feature_names, rotation=45)
            axes[0].set_ylabel('Normalized Value')
            axes[0].grid(True, alpha=0.3)

        # Plot 2: Value distribution
        if len(tensor.shape) == 3:
            for i, feature in enumerate(features[:3]):  # Show first 3 features
                feature_data = tensor[i].flatten()
                axes[1].hist(feature_data, bins=50, alpha=0.6, label=feature, density=True)
            axes[1].set_title('Value Distribution by Feature', fontsize=14, fontweight='bold')
            axes[1].set_xlabel('Normalized Value')
            axes[1].set_ylabel('Density')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)

        # Plot 3: Sparsity analysis
        if len(tensor.shape) == 3:
            sparsity_data = []
            for i, _feature in enumerate(features):
                feature_data = tensor[i]
                non_zero_ratio = np.count_nonzero(feature_data) / feature_data.size
                sparsity_data.append(non_zero_ratio)

            axes[2].bar(range(len(features)), sparsity_data, color='darkgreen', alpha=0.7)
            axes[2].set_title('Feature Density (Non-zero Ratio)', fontsize=14, fontweight='bold')
            axes[2].set_xticks(range(len(features)))
            axes[2].set_xticklabels(features, rotation=45)
            axes[2].set_ylabel('Non-zero Ratio')
            axes[2].set_ylim(0, 1)
            axes[2].grid(True, alpha=0.3)

        # Plot 4: Tensor properties
        properties = {
            'Shape': f"{tensor.shape}",
            'Size': f"{tensor.size:,}",
            'Memory (KB)': f"{tensor.nbytes/1024:.1f}",
            'Non-zeros': f"{np.count_nonzero(tensor):,}"
        }

        axes[3].text(0.1, 0.9, "Tensor Properties:", fontsize=14, fontweight='bold',
                    transform=axes[3].transAxes)

        for i, (key, value) in enumerate(properties.items()):
            axes[3].text(0.1, 0.8 - i*0.15, f"{key}: {value}", fontsize=12,
                        transform=axes[3].transAxes)

        axes[3].set_xlim(0, 1)
        axes[3].set_ylim(0, 1)
        axes[3].axis('off')

        plt.tight_layout()

        # Save summary plot
        summary_path = self.output_dir / "step3_feature_summary.png"
        fig.savefig(summary_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)

        self.summary_plot_path = summary_path

    def generate_report(self):
        """Generate unified HTML report."""
        self.log_section(
            "üìä Generating Unified Report",
            "Creating comprehensive HTML report with all results..."
        )

        # Create HTML report
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Represent Complete Workflow Demo Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; border-bottom: 3px solid #3498db; padding-bottom: 20px; }}
        h2 {{ color: #34495e; margin-top: 40px; padding: 15px; background: linear-gradient(135deg, #3498db, #2980b9); color: white; border-radius: 5px; }}
        .section {{ margin: 30px 0; padding: 20px; border-left: 4px solid #3498db; background: #f8f9fa; }}
        .timestamp {{ color: #7f8c8d; font-style: italic; float: right; }}
        .success {{ color: #27ae60; font-weight: bold; }}
        .error {{ color: #e74c3c; font-weight: bold; }}
        .info {{ color: #3498db; font-weight: bold; }}
        pre {{ background: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        img {{ max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }}
        .plot-container {{ text-align: center; margin: 30px 0; }}
        .summary {{ background: linear-gradient(135deg, #1abc9c, #16a085); color: white; padding: 25px; border-radius: 10px; margin: 30px 0; }}
        .grid {{ display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }}
        .metric {{ background: white; padding: 20px; border-radius: 5px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
        .metric-value {{ font-size: 2em; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ color: #7f8c8d; margin-top: 10px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üéØ Represent Complete Workflow Demo</h1>
        <div class="summary">
            <h3>üöÄ Three Core Modules Demonstration</h3>
            <p>This report showcases the complete represent workflow using all three core modules:</p>
            <ol>
                <li><strong>Global Threshold Calculator</strong> - Calculate consistent classification thresholds</li>
                <li><strong>Dataset Builder</strong> - Create comprehensive symbol datasets</li>
                <li><strong>Market Depth Processor</strong> - Generate and visualize market depth features</li>
            </ol>
            <p><em>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</em></p>
        </div>
"""

        # Add each section
        for section in self.report_sections:
            html_content += f"""
        <h2>{section['title']} <span class="timestamp">{section['timestamp']}</span></h2>
        <div class="section">
            <pre>{section['content']}</pre>
        </div>
"""

        # Add visualizations if they exist
        plot_sections = [
            ("step1_global_thresholds.png", "Global Thresholds Visualization"),
            ("step2_dataset_analysis.png", "Dataset Analysis"),
            ("step3_volume_feature.png", "Volume Feature"),
            ("step3_variance_feature.png", "Variance Feature"),
            ("step3_trade_counts_feature.png", "Trade Counts Feature"),
            ("step3_rgb_combination.png", "RGB Feature Combination"),
            ("step3_feature_summary.png", "Feature Summary Analysis")
        ]

        for plot_file, plot_title in plot_sections:
            plot_path = self.output_dir / plot_file
            if plot_path.exists():
                html_content += f"""
        <div class="plot-container">
            <h3>{plot_title}</h3>
            <img src="{plot_file}" alt="{plot_title}">
        </div>
"""

        # Add final summary
        html_content += """
        <div class="summary">
            <h3>‚úÖ Workflow Complete!</h3>
            <div class="grid">
                <div class="metric">
                    <div class="metric-value">3</div>
                    <div class="metric-label">Core Modules</div>
                </div>
                <div class="metric">
                    <div class="metric-value">100%</div>
                    <div class="metric-label">Success Rate</div>
                </div>
            </div>
            <p><strong>The complete represent workflow has been successfully demonstrated!</strong></p>
            <p>All three core modules worked together to:</p>
            <ul>
                <li>Calculate consistent classification thresholds across data</li>
                <li>Build comprehensive symbol datasets with uniform distributions</li>
                <li>Generate and visualize market depth features for ML training</li>
            </ul>
        </div>
    </div>
</body>
</html>
"""

        # Save HTML report
        report_path = self.output_dir / "complete_workflow_report.html"
        with open(report_path, 'w') as f:
            f.write(html_content)

        final_info = f"""
‚úÖ Complete workflow demo finished successfully!

üìä Report Summary:
   ‚Ä¢ Total sections: {len(self.report_sections)}
   ‚Ä¢ Visualizations created: {len(list(self.output_dir.glob('*.png')))}
   ‚Ä¢ Output directory: {self.output_dir}
   ‚Ä¢ HTML report: {report_path}

üéØ Three core modules demonstrated:
   1. Global Threshold Calculator ‚úÖ
   2. Dataset Builder ‚úÖ
   3. Market Depth Processor ‚úÖ

üìÅ All outputs saved to: {self.output_dir.absolute()}
üåê Open the HTML report to view complete results!
"""

        self.log_section("üéâ WORKFLOW COMPLETE", final_info)
        return report_path

    def demonstrate_compatible_configs(self):
        """Demonstrate the create_compatible_configs convenience function."""
        self.log_section(
            "üîó BONUS: Compatible Configuration Creation",
            "Demonstrating create_compatible_configs() - one function for all three modules..."
        )

        # Show the convenience function for creating compatible configs
        dataset_cfg, threshold_cfg, processor_cfg = create_compatible_configs(
            currency="EURUSD",
            features=["volume", "variance"],
            lookback_rows=2000,
            lookforward_input=1500,
            lookforward_offset=200,
            nbins=9,
            samples=30000,
            micro_pip_size=0.00001,
            jump_size=50
        )

        compatible_info = f"""
üîó CREATE_COMPATIBLE_CONFIGS() - One Function, Three Configs:

‚ú® Single function call creates all three focused configurations:
   create_compatible_configs(currency="EURUSD", features=["volume", "variance"], ...)

üìä DatasetBuilderConfig output:
   ‚Ä¢ Currency: {dataset_cfg.currency}
   ‚Ä¢ Lookback rows: {dataset_cfg.lookback_rows}
   ‚Ä¢ Lookforward input: {dataset_cfg.lookforward_input}
   ‚Ä¢ Lookforward offset: {dataset_cfg.lookforward_offset}

üéØ GlobalThresholdConfig output:
   ‚Ä¢ Currency: {threshold_cfg.currency}
   ‚Ä¢ Nbins: {threshold_cfg.nbins}
   ‚Ä¢ Sample fraction: {threshold_cfg.sample_fraction}
   ‚Ä¢ Jump size: {threshold_cfg.jump_size}

‚ö° MarketDepthProcessorConfig output:
   ‚Ä¢ Features: {processor_cfg.features}
   ‚Ä¢ Samples: {processor_cfg.samples:,}
   ‚Ä¢ Micro pip size: {processor_cfg.micro_pip_size}
   ‚Ä¢ Time bins: {processor_cfg.samples // processor_cfg.ticks_per_bin}

‚úÖ Compatibility guaranteed:
   ‚Ä¢ Shared parameters (currency, lookback, etc.) are synchronized
   ‚Ä¢ Currency-specific optimizations applied automatically
   ‚Ä¢ All three configs work together seamlessly

üöÄ Perfect for complex workflows requiring all three modules!
"""

        self.log_section("üîó Compatible Configuration Results", compatible_info)


def main():
    """Run the complete workflow demo."""
    print("üöÄ Starting Complete Workflow Demo")
    print("=" * 50)

    demo = WorkflowDemo()

    try:
        # Step 1: Global thresholds
        threshold_results = demo.step1_global_thresholds()

        # Step 2: Dataset building
        dataset_results = demo.step2_dataset_building(threshold_results)

        # Step 3: Market depth visualization
        demo.step3_market_depth_visualization(dataset_results)

        # Bonus: Demonstrate compatible configs
        demo.demonstrate_compatible_configs()

        # Generate final report
        report_path = demo.generate_report()

        print(f"\nüéâ Demo complete! Open: {report_path}")

    except Exception as e:
        print(f"\n‚ùå Demo failed: {str(e)}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
